{"cells": [{"cell_type": "code", "execution_count": 1, "id": "2b8bcc2e-1e3c-4cb8-86e1-1f6eb176bc50", "metadata": {}, "outputs": [], "source": "from pyspark.sql import SparkSession"}, {"cell_type": "code", "execution_count": 2, "id": "d3bc0063-00d9-4e64-969a-6f90edf1996b", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "24/03/12 01:38:03 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}], "source": "# Create SparkSession from builder\nimport pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.master(\"local\") \\\n                    .appName('project-pipeline') \\\n                    .getOrCreate()"}, {"cell_type": "code", "execution_count": 3, "id": "aaf153ed-b2e7-49a2-8592-950a431407ee", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "df = spark.read.csv('gs://peru-real-state-datalake/2024-03-10/*.csv', header=True, inferSchema=True, multiLine=True)"}, {"cell_type": "code", "execution_count": 4, "id": "0666cade-859b-4435-95a2-d28c4ac95892", "metadata": {}, "outputs": [], "source": "from pyspark.sql.functions import when\nfrom pyspark.sql.functions import expr\nfrom pyspark.sql.functions import regexp_replace\nfrom pyspark.sql.types import FloatType, IntegerType\nfrom pyspark.sql.functions import col, substring"}, {"cell_type": "code", "execution_count": 5, "id": "867e48c6-87d5-42b1-94cc-7cf515664fa6", "metadata": {}, "outputs": [], "source": "#Normalize some values in \"property_type\"\ndf = df.withColumn(\"property_type\", when(df[\"property_type\"] == \"casas\", \"casa\").otherwise(df[\"property_type\"]))\ndf = df.withColumn(\"property_type\", when(df[\"property_type\"] == \"departamentos\", \"departamento\").otherwise(df[\"property_type\"]))\ndf = df.withColumn(\"property_type\", when(df[\"property_type\"] == \"oficinas\", \"oficina\").otherwise(df[\"property_type\"]))\ndf = df.withColumn(\"property_type\", when(df[\"property_type\"] == \"casas-playa\", \"casa de playa\").otherwise(df[\"property_type\"]))\ndf = df.withColumn(\"property_type\", when(df[\"property_type\"] == \"locales-comerciales\", \"local\").otherwise(df[\"property_type\"]))\ndf = df.withColumn(\"property_type\", when(df[\"property_type\"] == \"casas-condominio\", \"condo\").otherwise(df[\"property_type\"]))\ndf = df.withColumn(\"property_type\", when(df[\"property_type\"] == \"terrenos\", \"terreno\").otherwise(df[\"property_type\"]))"}, {"cell_type": "code", "execution_count": 6, "id": "ca3c3fc5-8154-49be-8134-063f38031eaa", "metadata": {}, "outputs": [], "source": "#Clean the values in \"price\" column and convert them to float\nclean = df.withColumn('price', regexp_replace('price', '\\n', ''))\nclean = clean.withColumn('price', regexp_replace('price', ' ', ''))\nclean = clean.withColumn('price', regexp_replace('price', '\\\\$', ''))\nclean = clean.withColumn('price', regexp_replace('price', ',', ''))\nclean = clean.withColumn(\"price\", clean[\"price\"].cast(FloatType()))"}, {"cell_type": "code", "execution_count": 7, "id": "9c683b75-5830-4b83-9f84-1c964620e82a", "metadata": {}, "outputs": [], "source": "#Clean the values in \"size_m2\" and convert them to float\nclean = clean.withColumn(\"size_m2\", regexp_replace(\"size_m2\", \"m2\", \"\"))\nclean = clean.withColumn(\"size_m2\", clean[\"size_m2\"].cast(FloatType()))"}, {"cell_type": "code", "execution_count": 8, "id": "c44d1fc5-1f9a-4a2d-a850-f4b4fd1ef2d3", "metadata": {}, "outputs": [], "source": "#Clean some irregular values in \"city\"\nclean = clean.withColumn(\"city\", regexp_replace(\"city\", \"-departamento/list\", \"\"))"}, {"cell_type": "code", "execution_count": 9, "id": "31514bf0-838d-41c3-8b90-d6a8f3d06f90", "metadata": {}, "outputs": [], "source": "#Convert the coordinates to float\nclean = clean.withColumn(\"longitude_x\", clean[\"longitude_x\"].cast(FloatType()))\nclean = clean.withColumn(\"latitude_y\", clean[\"latitude_y\"].cast(FloatType()))"}, {"cell_type": "code", "execution_count": 10, "id": "42f1b182-3801-4562-a354-31d6f1756924", "metadata": {}, "outputs": [], "source": "#Delete the strings from \"rooms\" and \"bathrooms\" and convert them to integer\nclean = clean.withColumn(\"rooms\", regexp_replace(\"rooms\", \" Habitaciones\", \"\"))\nclean = clean.withColumn(\"bathrooms\", regexp_replace(\"rooms\", \" Ba\u00f1os\", \"\"))\nclean = clean.withColumn(\"rooms\", clean[\"rooms\"].cast(IntegerType()))\nclean = clean.withColumn(\"bathrooms\", clean[\"bathrooms\"].cast(IntegerType()))"}, {"cell_type": "code", "execution_count": 11, "id": "e15399bb-41df-4c00-8598-023adc17f539", "metadata": {}, "outputs": [], "source": "#Clean the empty spaces and the unnecesary characters in \"specs\"\nclean = clean.withColumn('specs', regexp_replace('specs', '\\n', ''))\nclean = clean.withColumn('specs', regexp_replace('specs', '\\\\s{2,}', ' '))\nclean = clean.withColumn('specs', substring(col('specs'), 2, 1000000))"}, {"cell_type": "code", "execution_count": 12, "id": "37d7ad7c-23e0-403a-b7a1-dcaf9fa4bd22", "metadata": {}, "outputs": [], "source": "#Delete the \"https://www.\" from the urls\nclean = clean.withColumn('url', regexp_replace(\"url\", \"https://www.\", \"\"))"}, {"cell_type": "code", "execution_count": null, "id": "5718b965-9ecc-4875-ad79-1783fd80116c", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 13, "id": "dafecb85-aa6a-45bc-99e0-52c3fe0d5936", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- scraped_on: timestamp (nullable = true)\n |-- city: string (nullable = true)\n |-- property_type: string (nullable = true)\n |-- operation: string (nullable = true)\n |-- name: string (nullable = true)\n |-- price: float (nullable = true)\n |-- longitude_x: float (nullable = true)\n |-- latitude_y: float (nullable = true)\n |-- size_m2: float (nullable = true)\n |-- rooms: integer (nullable = true)\n |-- bathrooms: integer (nullable = true)\n |-- adress: string (nullable = true)\n |-- description: string (nullable = true)\n |-- specs: string (nullable = true)\n |-- url: string (nullable = true)\n\n"}], "source": "clean.printSchema()"}, {"cell_type": "code", "execution_count": 14, "id": "ba01e725-2429-4610-bb14-4af084c8c7be", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "20932"}, "execution_count": 14, "metadata": {}, "output_type": "execute_result"}], "source": "clean.select(\"operation\").count()"}, {"cell_type": "code", "execution_count": 15, "id": "5bacd490-6f60-4d2b-aff0-3246ddf411ec", "metadata": {}, "outputs": [], "source": "drops = [\"city\", \"longitude_x\", \"latitude_y\", \"size_m2\", \"rooms\", \"bathrooms\", \"price\"] \nclean = clean.dropDuplicates(subset=drops)"}, {"cell_type": "code", "execution_count": 16, "id": "a9ec4a67-750b-4793-8672-ed447734d571", "metadata": {}, "outputs": [], "source": "fillnas = [\"size_m2\", \"price\", \"rooms\", \"bathrooms\"]\nclean = clean.fillna(0, subset=fillnas)"}, {"cell_type": "code", "execution_count": 17, "id": "251919f0-c15c-44bc-8ebe-d2a1d6f409de", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- scraped_on: timestamp (nullable = true)\n |-- city: string (nullable = true)\n |-- property_type: string (nullable = true)\n |-- operation: string (nullable = true)\n |-- name: string (nullable = true)\n |-- price: float (nullable = false)\n |-- longitude_x: float (nullable = true)\n |-- latitude_y: float (nullable = true)\n |-- size_m2: float (nullable = false)\n |-- rooms: integer (nullable = true)\n |-- bathrooms: integer (nullable = true)\n |-- adress: string (nullable = true)\n |-- description: string (nullable = true)\n |-- specs: string (nullable = true)\n |-- url: string (nullable = true)\n\n"}], "source": "clean.printSchema()"}, {"cell_type": "code", "execution_count": 20, "id": "07f56520-a870-4625-8332-6206db14cc91", "metadata": {}, "outputs": [], "source": "clean = clean.withColumn(\"price_per_m2\", expr(\"price / size_m2\"))"}, {"cell_type": "code", "execution_count": 24, "id": "abc558ba-5b7c-468c-a8a3-6258b13d03f0", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 17:=============================>                            (1 + 1) / 2]\r"}, {"ename": "Py4JJavaError", "evalue": "An error occurred while calling o233.showString.\n: org.apache.spark.SparkException: Job 11 cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1195)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1193)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1193)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2940)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$stop$3(DAGScheduler.scala:2834)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1485)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2834)\n\tat org.apache.spark.SparkContext.$anonfun$stop$11(SparkContext.scala:2158)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1485)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2158)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2112)\n\tat org.apache.spark.SparkContext.$anonfun$new$37(SparkContext.scala:710)\n\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2067)\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)", "\u001b[0;32m/tmp/ipykernel_26379/859929087.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n", "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o233.showString.\n: org.apache.spark.SparkException: Job 11 cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1195)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1193)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1193)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2940)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$stop$3(DAGScheduler.scala:2834)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1485)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2834)\n\tat org.apache.spark.SparkContext.$anonfun$stop$11(SparkContext.scala:2158)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1485)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2158)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2112)\n\tat org.apache.spark.SparkContext.$anonfun$new$37(SparkContext.scala:710)\n\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2067)\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"]}], "source": "clean.show(50)"}, {"cell_type": "code", "execution_count": 25, "id": "91f75136-96f1-4f77-8a80-a89469f5c059", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "ERROR:root:Exception while sending command.\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 516, in send_command\n    raise Py4JNetworkError(\"Answer from Java side is empty\")\npy4j.protocol.Py4JNetworkError: Answer from Java side is empty\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n    response = connection.send_command(command)\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 539, in send_command\n    raise Py4JNetworkError(\npy4j.protocol.Py4JNetworkError: Error while sending or receiving\n"}, {"ename": "Py4JError", "evalue": "An error occurred while calling o233.count", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)", "\u001b[0;32m/tmp/ipykernel_26379/2678694537.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m         \"\"\"\n\u001b[0;32m--> 804\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    332\u001b[0m                     format(target_id, \".\", name, value))\n\u001b[1;32m    333\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             raise Py4JError(\n\u001b[0m\u001b[1;32m    335\u001b[0m                 \u001b[0;34m\"An error occurred while calling {0}{1}{2}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m                 format(target_id, \".\", name))\n", "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o233.count"]}], "source": "clean.count()"}, {"cell_type": "code", "execution_count": null, "id": "b84584f5-3d22-48bd-94ca-dba43b9a34c1", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}}, "nbformat": 4, "nbformat_minor": 5}